{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747bf3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "from scipy import stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import holidays\n",
    "import statsmodels.formula.api as smf\n",
    "pd.options.display.max_rows = 50\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94a9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extraneous Analysis\n",
    "# #Initial Data Cleaning from Model 4\n",
    "df = pd.read_csv(\"ben_intraday_data.csv\")\n",
    "df['str_date'] = df['date']\n",
    "df.drop('date',axis=1)\n",
    "df['datetime'] = df['date'] + ' ' + df['time']\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.index = pd.DatetimeIndex(df['datetime'])\n",
    "trading_days = df.date.unique()\n",
    "\n",
    "# df = df.fillna('')\n",
    "# df['symbol'] = df['sym_root'] + df['sym_suffix']\n",
    "# Sector = df.loc[(df.sym_root == 'NFLX') | (df.sym_root == 'PYPL') | (df.sym_root == 'NFLX') | (df.sym_root == 'MSFT')]\n",
    "# S = Sector.groupby(['datetime','symbol']).sum().groupby('datetime').sum()\n",
    "# S = S.between_time('09:31:00', '15:59:00').sort_index()\n",
    "\n",
    "# MSFT = Sector.loc[Sector.symbol == 'MSFT']\n",
    "# MSFT.index = pd.DatetimeIndex(MSFT['datetime'])\n",
    "# MSFT = MSFT.between_time('09:31:00', '15:59:00').sort_index()\n",
    "# MSFT['cum_vol'] = np.nan\n",
    "# MSFT = MSFT.resample('60s').asfreq()\n",
    "# MSFT['size'].fillna(0, inplace=True)\n",
    "# MSFT['date'] = MSFT.index.date\n",
    "# MSFT['time'] = MSFT.index.time\n",
    "# MSFT['date'] = pd.to_datetime(MSFT.date) \n",
    "# MSFT = MSFT.loc[MSFT.date.isin(rel), :]\n",
    "# MSFT = MSFT.between_time('09:31:00', '15:59:00')\n",
    "# S['cum_vol'] = np.nan\n",
    "# S = S.resample('60s').asfreq()\n",
    "# S['size'].fillna(0, inplace=True)\n",
    "# S['date'] = S.index.date\n",
    "# S['time'] = S.index.time\n",
    "# S['date'] = pd.to_datetime(S.date) \n",
    "# S = S.loc[S.date.isin(trading_days), :]\n",
    "# S = S.between_time('09:31:00', '15:59:00')\n",
    "# trading_days = list(MSFT.date.unique())\n",
    "# S['date'] = pd.to_datetime(S.index) \n",
    "# S['date'] = S['date'].dt.date\n",
    "# S['date'] = pd.to_datetime(S.date) \n",
    "# for day in trading_days:\n",
    "#     S.loc[day==S.date, 'cum_vol'] = S.loc[day== S.date,'size'].cumsum()\n",
    "#     MSFT.loc[day==MSFT.date, 'cum_vol'] = MSFT.loc[day== MSFT.date,'size'].cumsum()\n",
    "# for days in trading_days:\n",
    "#     num = MSFT['cum_vol'][MSFT.date == days].values[-1]\n",
    "#     num1 = S['cum_vol'][S.date == days].values[-1]\n",
    "#     S.loc[days==S.date, 'Correction'] = num1\n",
    "#     MSFT.loc[days==MSFT.date, 'Correction'] = num\n",
    "# MSFT['CDF'] = MSFT['cum_vol']/MSFT['Correction']\n",
    "# S['CDF'] = S['cum_vol']/S['Correction']\n",
    "# x = S.CDF\n",
    "# y = MSFT.CDF\n",
    "# np.corrcoef(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e699c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A rather inefficient way to get the trading days, but it works\n",
    "# Gives the days until June 1st, 2019 (can be adjusted easily)\n",
    "K = pd.Series(trading_days)\n",
    "deys = pd.DataFrame({'Dates':K.values})\n",
    "rel = deys.loc[inept.Dates < datetime(2019,6,1)].Dates.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54898842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dataframe, Total, with all of the S&P 500 Stocks \n",
    "# Produces a datetime and symbol column\n",
    "Total = pd.DataFrame()\n",
    "for day in rel:\n",
    "    cur = pd.Timestamp(day).strftime(\"%Y%m%d\")\n",
    "    csvFile = \"/Users/bengoldman/Desktop/Junior Year/Data Science Clinic/SP500/SP500_\" + str(cur) + \".csv\"\n",
    "    partial = pd.read_csv(csvFile)\n",
    "    Total = pd.concat([Total,partial])\n",
    "Total = Total.reset_index(drop = True)\n",
    "Total['str_date'] = Total['date']\n",
    "Total.drop('date',axis=1)\n",
    "Total['datetime'] = Total['date'] + ' ' + Total['time']\n",
    "Total['datetime'] = pd.to_datetime(Total['datetime'])\n",
    "Total['date'] = pd.to_datetime(Total['date'])\n",
    "# df.index = pd.DatetimeIndex(df['datetime'])\n",
    "Total= Total.fillna('')\n",
    "Total['symbol'] = Total['sym_root'] + Total['sym_suffix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204f9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include a sector column in the Total dataframe\n",
    "payload=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "first_table = payload[0]\n",
    "use = first_table[['Symbol','GICS Sector']]\n",
    "use.set_index\n",
    "use = use.rename(columns={\"GICS Sector\": \"Sector\"})\n",
    "use = use.set_index('Symbol')\n",
    "Total['Sector'] = Total.sym_root.map(use.to_dict()['Sector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1323c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraneous Code for Testing\n",
    "# Tech = Total.loc[Total.Sector == 'Information Technology'].groupby(['datetime','symbol']).sum().groupby('datetime').sum()\n",
    "# Tech = Tech.between_time('09:31:00', '15:59:00').sort_index()\n",
    "# Tech['cum_vol'] = np.nan\n",
    "# Tech = Tech.resample('60s').asfreq()\n",
    "# Tech['size'].fillna(0, inplace=True)\n",
    "# Tech['date'] = Tech.index.date\n",
    "# Tech['time'] = Tech.index.time\n",
    "# Tech['date'] = pd.to_datetime(Tech.date) \n",
    "# Tech = Tech.loc[Tech.date.isin(rel), :]\n",
    "# Tech = Tech.between_time('09:31:00', '15:59:00')\n",
    "# Tech['date'] = pd.to_datetime(Tech.index) \n",
    "# Tech['date'] = Tech['date'].dt.date\n",
    "# Tech['date'] = pd.to_datetime(Tech.date) \n",
    "# for day in rel:\n",
    "#     Tech.loc[day==Tech.date, 'cum_vol'] = Tech.loc[day== Tech.date,'size'].cumsum()\n",
    "# for days in rel:\n",
    "#     num = Tech['cum_vol'][Tech.date == days].values[-1]\n",
    "#     Tech.loc[days==Tech.date, 'Correction'] = num\n",
    "# Tech['CDF'] = Tech['cum_vol']/Tech['Correction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c7b1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a stock with days in the relative time period (see above)\n",
    "## Output the stock with upsampling and\n",
    "def clean_data_distribution(stock):\n",
    "    cur = stock.between_time('09:31:00', '15:59:00').sort_index()\n",
    "    cur['cum_vol'] = np.nan\n",
    "    cur = cur.resample('60s').asfreq()\n",
    "    cur['size'].fillna(0, inplace=True)\n",
    "    cur['date'] = cur.index.date\n",
    "    cur['time'] = cur.index.time\n",
    "    cur['date'] = pd.to_datetime(cur.date) \n",
    "    cur = cur.loc[cur.date.isin(rel), :]\n",
    "    cur = cur.between_time('09:31:00', '15:59:00')\n",
    "    cur['date'] = pd.to_datetime(cur.index) \n",
    "    cur['date'] = cur['date'].dt.date\n",
    "    cur['date'] = pd.to_datetime(cur.date) \n",
    "    for day in rel:\n",
    "        cur.loc[day==cur.date, 'cum_vol'] = cur.loc[day== cur.date,'size'].cumsum()\n",
    "    for days in rel:\n",
    "        num = cur['cum_vol'][cur.date == days].values[-1]\n",
    "        cur.loc[days==cur.date, 'Correction'] = num\n",
    "    cur['CDF'] = cur['cum_vol']/cur['Correction']\n",
    "    cur['PDF'] = cur['size']/cur['Correction']\n",
    "    return cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd0c2629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a sector dataframe with intraday volume at each minute\n",
    "def get_sector_data(Sec):\n",
    "    Tech = Total.loc[Total.Sector == Sec].groupby(['datetime','symbol']).sum().groupby('datetime').sum()\n",
    "    Tech = Tech.between_time('09:31:00', '15:59:00').sort_index()\n",
    "    return Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "082b5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates each of the sector dataframes and the SPY dataframe\n",
    "Tech = clean_data_distribution(get_sector_data(\"Information Technology\"))\n",
    "Tech = Tech.rename(columns = {Tech.columns[5]: 'CDF_SEC', Tech.columns[6]: 'PDF_SEC'})\n",
    "Fin = clean_data_distribution(get_sector_data(\"Financials\"))\n",
    "Fin = Fin.rename(columns = {Fin.columns[5]: 'CDF_SEC', Fin.columns[6]: 'PDF_SEC'})\n",
    "CD = clean_data_distribution(get_sector_data(\"Consumer Discretionary\"))\n",
    "CD = CD.rename(columns = {CD.columns[5]: 'CDF_SEC', CD.columns[6]: 'PDF_SEC'})\n",
    "CS = clean_data_distribution(get_sector_data(\"Communication Services\"))\n",
    "CD = CS.rename(columns = {CS.columns[5]: 'CDF_SEC', CS.columns[6]: 'PDF_SEC'})\n",
    "IN = clean_data_distribution(get_sector_data(\"Industrials\"))\n",
    "IN = CD.rename(columns = {IN.columns[5]: 'CDF_SEC', IN.columns[6]: 'PDF_SEC'})\n",
    "HC = clean_data_distribution(get_sector_data(\"Health Care\"))\n",
    "HC = HC.rename(columns = {HC.columns[5]: 'CDF_SEC', HC.columns[6]: 'PDF_SEC'})\n",
    "NRG = clean_data_distribution(get_sector_data(\"Energy\"))\n",
    "NRG = NRG.rename(columns = {NRG.columns[5]: 'CDF_SEC', NRG.columns[6]: 'PDF_SEC'})\n",
    "\n",
    "MAT = clean_data_distribution(get_sector_data(\"Materials\"))\n",
    "MAT = MAT.rename(columns = {MAT.columns[5]: 'CDF_SEC', MAT.columns[6]: 'PDF_SEC'})\n",
    "\n",
    "UTIL = clean_data_distribution(get_sector_data(\"Utilities\"))\n",
    "UTIL = UTIL.rename(columns = {UTIL.columns[5]: 'CDF_SEC', UTIL.columns[6]: 'PDF_SEC'})\n",
    "\n",
    "RE = clean_data_distribution(get_sector_data(\"Real Estate\"))\n",
    "RE = RE.rename(columns = {RE.columns[5]: 'CDF_SEC', RE.columns[6]: 'PDF_SEC'})\n",
    "\n",
    "SPY = Total.loc[Total.symbol == 'SPY']\n",
    "SPY.index = pd.DatetimeIndex(SPY['datetime'])\n",
    "SPY = clean_data_distribution(SPY)\n",
    "SPY = SPY.rename(columns = {SPY.columns[11]: 'CDF_CTRL', SPY.columns[12]: 'PDF_CTRL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02f47e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces a scatter plot for each sector using the SPY as a control\n",
    "def produce_scatter(sector, sector_rep):\n",
    "    plt.xlabel('SPY Index Coefficient')\n",
    "    plt.ylabel('Sector Coefficient')\n",
    "    plt.title(\"Sector Versus Market Index \" + sector)\n",
    "    enum = Total.loc[Total.Sector == sector].symbol.unique()\n",
    "    ctrl = SPY.PDF_CTRL\n",
    "    sect = sector_rep.PDF_SEC\n",
    "    for scks in enum:\n",
    "        try:\n",
    "            cur = Total.loc[Total.symbol == scks]\n",
    "            cur.index = pd.DatetimeIndex(cur['datetime'])\n",
    "            cur = cur.between_time('09:31:00', '15:59:00').sort_index()\n",
    "            cur = clean_data_distribution(cur)\n",
    "            stock= cur.PDF\n",
    "            model = pd.concat([sect,ctrl,stock],axis=1)\n",
    "            result = smf.ols(formula = \"stock ~ ctrl + sect\",data = model).fit()\n",
    "            params = result.params\n",
    "            sect_coef = params[2]\n",
    "            spy_coef = params[1]\n",
    "            plt.scatter(spy_coef,sect_coef)\n",
    "            plt.annotate(scks,(spy_coef,sect_coef),fontsize=3)\n",
    "        except:\n",
    "            pass\n",
    "    plt.savefig(sector + 'ScatterPl.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "494b52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce_scatter(\"Health Care\", HC)\n",
    "# MRNA = MRNA.loc[Total.symbol == 'MRNA']\n",
    "# MRNA.index = pd.DatetimeIndex(LLY['datetime'])\n",
    "# MRNA = MRNA.between_time('09:31:00', '15:59:00').sort_index()\n",
    "# MRNA= clean_data_distribution(MRNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63752ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the SPY, Stock, and Sector over a fixed interval with a given lag\n",
    "def minute_test(ticker,Sect, minute1,minute2,lag):\n",
    "    stock = Total.loc[Total.symbol == ticker]\n",
    "    stock.index = pd.DatetimeIndex(stock['datetime'])\n",
    "    stock = stock.between_time('09:31:00', '15:59:00').sort_index()\n",
    "    stock= clean_data_distribution(stock)\n",
    "    minutes = Sect.time.unique()\n",
    "    avg_stock = pd.Series(dtype='float64')\n",
    "    avg_Sector = pd.Series(dtype='float64')\n",
    "    avg_SPY = pd.Series(dtype='float64')\n",
    "    for minute in minutes:\n",
    "        avg_stock.loc[minute] = stock.loc[stock.index.time == minute, 'PDF'].mean()\n",
    "        avg_Sector.loc[minute] = Sect.loc[Sect.index.time == minute, 'PDF_SEC'].mean()\n",
    "        avg_SPY.loc[minute] = SPY.loc[SPY.index.time == minute, 'PDF_CTRL'].mean()\n",
    "    plt.plot(avg_stock.reset_index(drop=True)[minute1 - 1:minute2],label=ticker)\n",
    "    plt.plot(avg_SPY.reset_index(drop=True)[minute1 - 1:minute2],label = \"SPY\")\n",
    "    plt.plot(avg_Sector.reset_index(drop=True).shift(periods=lag)[minute1 - 1:minute2],label = \"Sector\")\n",
    "    plt.legend()\n",
    "    plt.title(\"PDF Curves for Sector with lag = \" + str(-lag) + \" and \" + ticker)\n",
    "    plt.xlabel(\"Minute\")\n",
    "    plt.ylabel(\"Percentage of Volume Traded\")\n",
    "    plt.savefig(str(minute2 - minute1) +'_Minutes_' + ticker + 'lag=' + str(-lag) + '.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d871e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a regression on the stock volume based on stock and sector volumes from previous minutes (lag)\n",
    "def lag_regression(ticker,Sect,lag):\n",
    "    stock = Total.loc[Total.symbol == ticker]\n",
    "    stock.index = pd.DatetimeIndex(stock['datetime'])\n",
    "    stock = stock.between_time('09:31:00', '15:59:00').sort_index()\n",
    "    stock= clean_data_distribution(stock)\n",
    "    stck = stock.PDF[::-1 * lag]\n",
    "    ctrl = stock.shift(periods = -lag).PDF[::-1 * lag]\n",
    "    sect=Sect.PDF_SEC.shift(periods = -lag)[::-1 * lag]\n",
    "    model = pd.concat([sect,stck,ctrl],axis=1)\n",
    "    result = smf.ols(formula = \"stck ~ sect + ctrl\",data = model).fit()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "638cf3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a regression on the stock volume based on sector volumes from previous minutes (lag)\n",
    "def lag_regression_without(ticker,Sect,lag):\n",
    "    stock = Total.loc[Total.symbol == ticker]\n",
    "    stock.index = pd.DatetimeIndex(stock['datetime'])\n",
    "    stock = stock.between_time('09:31:00', '15:59:00').sort_index()\n",
    "    stock= clean_data_distribution(stock)\n",
    "    stck = stock.PDF[::-1 * lag]\n",
    "    ctrl = stock.shift(periods = -lag).PDF[::-1 * lag]\n",
    "    sect=Sect.PDF_SEC.shift(periods = -lag)[::-1 * lag]\n",
    "    model = pd.concat([sect,stck,ctrl],axis=1)\n",
    "    result = smf.ols(formula = \"stck ~ ctrl\",data = model).fit()\n",
    "#     print(result.summary())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c259fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of the coefficients from the lag_regression method across all stocks in a given sector\n",
    "# We use the try function since not all of the stocks in the current S&P500 existed during the testing window\n",
    "def produce_lag_scatter(sector, sector_rep, lag):\n",
    "    plt.xlabel('Lag Term for Stock')\n",
    "    plt.ylabel('Lag Term for Sector')\n",
    "    plt.title(\"Sector Versus Market Index \" + sector + 'with lag = ' + str(lag))\n",
    "    enum = Total.loc[Total.Sector == sector].symbol.unique()\n",
    "    for scks in enum:\n",
    "        try:\n",
    "            result = lag_regression(scks,sector_rep,lag)\n",
    "            params = result.params\n",
    "            sect_coef = params[2]\n",
    "            ctrl_coef = params[1]\n",
    "            plt.scatter(ctrl_coef,sect_coef)\n",
    "            plt.annotate(scks,(ctrl_coef,sect_coef),fontsize=3)\n",
    "        except:\n",
    "            pass\n",
    "    plt.savefig(sector + 'Lag' + str(lag)+ '_regression.png', dpi=300)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
